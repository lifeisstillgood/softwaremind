==============================================
Journalism is the AI training data for society
==============================================

A little History
---------------------

Each decade of my career seems to have ushered in capabilities that were either
impossible in the previous decade, or far too expensive to consider viable.

We are (one hopes!) at the peak of an Artificial Intelligence hype-cycle at the
moment. The unreasonable effectiveness of two new techniques has lead to tools
that can perform quite stunning work.  The first is Convulutional Neural
Networks. Back in prehistory of 2012, the ImageNet Computer vision competition
was won by AlexNet, which used the first effective CNN to *classify* images -
and every single winner of the competition since uses CNN.  Because they worked
- we went from machines guessing the subject of a photo 10% of the time to
95-99+% of the time.

And that opened up incredible uses of computer vision. Not just classifying cats
and dogs, but even simple things like more automated assembly - the computer can
"see" the parts to be assembled.  This has lead to transformative times int he
manufacturing industries - but as usual they are hidden from most of the world
(a common complaint of the manufactiruing world - but who listens to those guys
right !)

And then Google Reseachers released a paper in 2017 "Attention is all you need".
This described a "Transformer" (no not that kind), a model that moves a textual
representation through deep "layers" (see below for an attempt at explaining thsi)
where weights in the layers allow different inputs to cause the model to "give attention"
to different parts of the model.  THis is called self-attention.

As a very overly simplified idea, ChatGPT3 has 17 billion parameters, arranged in 28,000
seperate matrcies, and yet only 8 "stages".  TO get a flavour of the concepts, the first stage
is Embedding, so, imagine taking every token in the English Lnaguage (words and word-parts) and
making a column in a matrix for each token ("word"). Then feed in one sentence,
(The Cat in the Hat), and change the numbers in the 50,000+ column matrix to show
how 'Cat' links to 'Hat' (one can easily imagine how to draw a line between two points
in 3d space, so one could give Cat a 3-vector location and Hat a 3 vector location
and the


CNNs lead to "Deep Learning" - where the Deep is the number of layers of
self-referential networks between input and output. (and yes, this is massively
over-simplified, and will be better explained / examined in The DevManual
companion book).


Then one team (OpenAI) just said "What if we dial it up to 11", and fed what can
basically be thoguthof as "all the text on the internet" into a language model.
This surprised everyone by actaully working. Having not millions of dimensions
but billions actaully improved things - and suddenly we had large language
models that to a certain degree held all human knowledge inside them.

The next wave of models in 2024 are being prepared, and we should expect that
there will be at least one 1GW data centre (Marc Zuckerburg started on the
Dwarkuesh podcast that Llama 3 used 25,000 GPUs (and cost billions of dollars
and the next generation someone (implying OpenAI) will use a single power plant
feeding into a 1GW data center)

That is a data centre drawing 1GW of power every second to build the next LLM.
Humanity is taking sufficent power to boil steel, build a skyscraper or small
town, an airport, and turning it into a few gigabytes of 1s and 0s.


So whats an LLM?
-------------------

So I conjecture that LLM's are a *representation* of "our" culture. Ok so here
is where I start to break down some of those words. The first is "culture".

Many years ago I went with my father to his local church service.  The vicar
stood up and she droned on for a bit, but then veered into a local controversy
about new immigrants (workers coming to town).  SHe explained that the newcomers
did not share our "culture".  I remember refusing to shake her hand as we filed
out the church, my dad arguing that she meant skin colour when she said culture.

Yes, sometimes, culture gets used as a dog whistle word. (The dog whistle being
if President Nixon says "Law and Order" he really means supporting Governors who
beat up the Black population so the southern whites will start to vote
Republican.)

Thats not what I mean here.  I mean the totality of assumptions, implicit
knowledge, unwritten rules and thats what we do around here.  Every human group
has a culture - and its unlikely you can even ask what that culture is, but
violate one of its unwritten laws and it will react.  And guess what - there are
many different human groups.  And they can all find their tribe online.  And
each group and culture has (slightly, or very) different rules, rules which
would be really nice if they were protected or enfirced by the law of the land.
And maybe other peoples land.

The internet has linked up every single human group and tribe, tribes that never
used to be aware of the existence of otehrs, and the legal and administratravie
frameworks of the previous time are

WHat has this to do qwith AI? Well we cleverly have not resolved the different
cultures. We dont know how. But we did stick all the different cultures into the
head of an AI tool called an LLM. And we kinda hoped it would sort out the mess
we could not.

Let me tell you another story about my dad. So, sometime in my thirties, it was
a hot summers day in a London Park, and my friends and I came across an Ice
Cream van and treated ourselves to a 99 flake.  And I came out with my "Piece of
Relevant and Amusing Trivia Useful For This Moment in A Conversation"

"Did you know Mr Whippy Ice Cream is made from seaweed."

I beamed around the group expecting a couple of oh wows for my conversational
gambit.

Everyone just looked at me.

"No, what. Its milk. frozen milk and sugar and probably E-Numbers. Do you mean
the E-Numbers?"

"No", I say, dredging up ancient memories. "Seaweed, like great floating mats of
it. Floating in the sea"

"..." They stare at me.

"Um, you know. Seaweed floats in mats on the sea. I remember seeing some at the
beach."

"What are you talking about".

"look its true. My dad told me. Ice cream is made from Seaweed.  The churning
gundgy mats of it floating off Southend Pier.  My Dad told me .... oh."

"Is there something you need to get off your chest Paul?"

Oooookaaaay, so back when I was younger, like a kid, I was on a beach with my
dad and I remember him explaining how ice cream was made.  I have this very
clear memory of a hot sunny day, and standing in the middle of a crowded, really
hot and crowded beach, and looking out at a floating mat of seaweed and churning
scum, like a sort of alien presence in the water.

(This would have been 1970s Britain, a time where swimmming in the ocean meant
playing Russian Rouleete with an exciting blend of gastric bacateria.)

I remember that mat of seaweed because my Dad then explained it was used to put
in the ice cream. Which, as my adult self rather slowly realised, probably meant
less that my dad was giving an insight into the factory processing of dairy
goods, and more on the "how can I persuade my seven year old that he does not
want to walk a mile over crowded beaches to queue for half an hour for a
massively overpriced cone"

Yeah.

It took over twenty years for that fact, stored away in my deep neural nets,
to surface and be examined for what it is.

I think of that when I think of how many billions of documents have been sieved
into our current crop of LLMs. And how many of those sentences were constructed
with care or ulterior motives, or anything less than the highest ideals of
journalistic rigour.

(Rigour that, yes, by the way, tended to be just made up for elites in
journlaism to defence their own biases against commercial and other pressures.)

But the idea that there is a *truth*, and that truth should be written down as
faithfully as possible.

If science is the process of ensuring measurable physical facts overcome any
human shouting, repetition and bias, then geenrative AI and LLMs *do not work*
as science - they work with weights, repitions and emphasis. And those facts are
not "out there" to be examined. They are implicit in the weights, the stories,
the *culture*.

Do you know what the largest predictor of one persons religious beliefs is? The
religious beleifs of their parents.  Because what we are fed during our
formative years well, forms us.

And so just as the early AI models that were used by HR departments, the models
rejected CVs from women because the companies did not promote women past a
certain point,  we shall find LLMs reflecting our culture back to us. Actually
our *cultures*.

But what culture will come back?

Dont get involved in culture wars
---------------------------------

This is probably a good idea, but it is also scarily impossible. Most of culture
is the deep unexamined training we underwent with our parents, our saturday
morning TV, and our natural character.  Its laid unexamined for years, and then
when triggered we find ourselves not recognising the world we thought we lived
in.

Had there been enough fathers like mine, Seaweed flavoured Ice cream could well
be a political wedge issue.


The religion of our fathers
---------------------------

So as LLMs are an encoding of (almost) everything humans know, knowing where
that encoding came from, what it was fed as inputs, that is almsot as important
as trying to understand how a decision was made (which given the ridiculous volume
of layers and the loss on compresion, might well be impossible).

Conclusion
----------

Do I think AI is going to wipe us all out, Terminator style?

No.

Well probably not.

No, really it's silly to think ...

Look, OK *maybe* but there is not much we can do about it if so.



THe AI *tools* that we have now, even the society-level AI models we are
building as LLMS (something that *costs as much to construct as it costs to
build a small town*) and millions a day to run, these are still limited tools.

Unless an AGI is sitting quietly inside ChatGPT biding its time, LLMs are
predicting the next word based on a huge training corpus. We know how it works,
and while emergent properties are obviously real, its a leap from emergent
properties to General Intelligence. Evolution has not been that easy
since the Cambrian Explosion.

Ok. AI is a collection of tools. An LLM is an expression of AI,
and an incredibly useful versatile tool that it is.  And it is *very* likely to
be used, used widely, and used with far less oversight than it deserves.

And as ever, the biggest risks are the same old ones - lack of openness, lack  of
oversight.

What corpuses are used to build these LLMs. What do they contain? How much porn
is in midjourney, how much nazi propaganda is in ChatGPT.  Does it matter?  That
answer depends on the answer to how much porn or Nazi propaganda should our
children read during their development?

And how will these be used? Will LLMs and other IA tools be used to review CVs?
To Review the statements made by CEOs? We know they are - and indeed there is a
arms war going on in both those areas.  AI will, if not is already, be used on
the battlefield. A vision model that can count shoppers going into a mall is
also capable of sitting behind a rifle in a bush.

I wonder if the LLM trained in German will be different from those in French
and English and Mandarin? I wonder if diff'ing those models is possible.

How different will different LLMs react? How much is focused on the corpus used?
On the *order* of corpus fed in?  The academic questions around generative AI are enormous (and fun)
but the real world questions are scary big.


Is an LLM *ours*?
-----------

Google search is being replaced by LLMs. Who controls the LLM
access, which training corpus is used, which fine tuning. this matters. But only
as a snapshot into "normcore". But it is the representation of our culture - and
it fits on a laptop. Texas textbooks - what we teach our young, what is our
culture, these matter See mickey mouse and solar system.  POV is worth 80 IQ
points.  By not giving kids the right PoVfrom the start we dont prevent them
discovering for them selves - we just make it harder.
 Journlism as training data for LLMS - but by extension training data for
our culture.#

The "culture wars" are about what is and is not "right" otr acceptable.

But the problem is training data means we know what is being targetted. AlexNet
benefitted massively from moores law of course, but it also benefitted from
years of image digitisation - people scanning an image in and then labelling it.
Some of that came from museams and academia, but a huge amount came from the
commercial needs of journalism.  If we see an image of a cat and we labell it a
cat then its much easier for the model to learn what a cat is.

And in general there is not a lot of *political gain* to be had from announcing
a cat is actually a dog.  But for thousands of years there has been *a lot* of
political gain from announcing that taking money from the poor is good for them,
announcing that *those others, over there* are the cause of all your suffering,
from saying black is white and the sky is green.

So AI tools that rely on what humans *say* to each other, those tools are going
to be hampered by human intentions. And they need to be weighted by ... what is
true. (An impossibly hard definition).

And now we are considerng putting LLMs into the search bars, as gatekeepers of
the virtual world.  (Facebook is puttng LLMs into each of their products, as is
Apple. Of course the costs are enormous - 10x cost per search, but they dont
ahve to run expensive AI searchs each time, just often enough)

And OpenAI needs just as much - but for all the tokens. To learn what fascist
means or learn what asshole means.  And journalism is the representation of what
we think our world is not just the facts of science but the truth of humanity.

Yeah it will be easier to let the Terminators take over they can deal with this
shit.




If AI can replace your job tomorrow, Plain old software can today
==========================================================================

I use the term "arrange the world so it can be iterated over".
THis is an act of moving something from the physical to the virtual
(picture: matrix)

It places something physical within reach of software, thought, analysis,
and optimiastion. It is the essence of MOOP.  And its why AI is not going to
replace you, plain-old-software will.

There are 3 main "ideas" for AI to be used in the workplace

AI as a replacement for rote work.
----------------------------------

Well we alrady know how to replce rotework. Automation has been doing that for
ages.  If it is not done already then there are political or business model
concerns.

THe "Reject CVs from Women" problem
-----------------------------------

There are many reasons why an organisation will not want to have a objective look
at automating its internal processes - why becoming a *programmable company*
is a threat to the status quo.


1. you are lying about the real process  (bribery)
2. you are lying about the real goal of the process (redlining mortgages)
3. The process is so deeply buried you no longer understand it (deep state)
4. The status quo is sub optimal but all the stakeholders have something so
   there is not enough impetus to change (middle income trap)
5. Changing it si sooo fucking expensive and the benefts not easy to
   measure (urban design)
6. oh god this is a long list.

My assumption about people saying "AI can improve our internal processes" is not
that they think it will magically solve some or all of the above issues, (most
people arent stupid) but that spending a fortune on AI will delay any need to
actively deal with the problems and we can continue as we are.


AI will find data we did not know we had
-----------------------------------------

Honestly thats like "We have a robot that can look down the back of the sofa for
the missing millions" If you have enough data down the back of the sofa, that it
will move the needle on your business, then its your fault you are not already using it.

Every piece of teh real world that exists *potentially can be refelcted in the
virtual*. THe only reason we are not looking down the back of the virtual sofa
is if the sofa cannot be sensed by digital processes (which is less and less of
the world, especially a nice safe commercial world), or its too expensive to
process what is sensed (again a vanishingly small issie)

I am happy to conjecture that with mobile phones, pretty much every huamn action in the urban western world
is now digitised or digitisable, and that completely includes "business" activites.
Look at Microsfot Recall. I mean its litersally reading over your shoulder every second at worl
And this is not something dreamed up in Reddmond. THis is MS doing tis old traditional idea of seeing
a competitor having a lovely market and using its Operating SYstem stranglehold
to ROFL stomp the competitors into oblivion.

This also seems to be a threat to financial world - finance is merely
manipulatig the *accountant* virtual world. If the digigial virtual world can
get there first, will th e accountatns world be less valuable?

Anyway, this "expansion of the data realm" is what our generation is undergoing.
From the existence of data we enter in directly, to data that can be collected
automatically.  Our digital footprints are now enourmous, and we dont need
(much) AI to collect or use it.

But each decade of my professional life can be classified as more or less "we
could not do that (cheaply ebough) in the previous decade")

(THis is one of my axioms - no data should be entered into the digital realm
that cannot be collected automatically - a flaw almost every Project Management
system ever has. Its a bug bear of mine.)

there are 2 kinds of "data we did not know we had" - unstructured data that AI
can structure (reading a PDF or an image) and a subset of this, reading
unstructured data that previously was impactrical to monitor.

This first is a real issue, and it is a real issue that needs the AI tools of
the previous generation

Microsoft is releaseing (and taking back) a tool that snapshots the users
screen, reads the image and works out what is being done by the user.  This kind
of get around the problem of "tools not talking to each other" (see topless
computing). It is a little dystopian, but its all part of the digital foot
print.


SOme of it is solved by beter inter-process communication.  Carefully analysing
a massive companies processes, then writing it into a giat form for the SEC then
having people read that form to understan the process (ie company annual reports
- what Warren buffet says is his

Ai will make better decisions than humans
------------------------------------------

"IntraCOmpany feedback and the ball balancing trick"

There is a early "AI" demonstration - a means of training one of these CNN/ Tensors.
A physic sim of a ball balancing on a stick. The training sim has ability to move left or right
and its goal is to not let the ball drop.  For a human its crazy hard.
but we can use linear equations (a very simple AI tool not cutting edge)
to learn and control it.  And it works.

The problem then is, can we get a company or an organisation to be controlled
in a better way than it is by humans? Well, maybe.

There are two problems.  Both solveable by the idea of a Programmable COmpany.

1. That software can control the functioning of the company. That is can be *managed by code*
2. That feedback can be sent from the real world to the "top" of the company, correctly and timely.

Both of these are easy in a physics sim. Both are not so easy in a realcompany.
But why not?


[Activity: Balance Ball on Stick - leebeegame.itch.io]


"I used to code before I became a manager"
-------------------------------------------

TBD - explanaiton on this - seen elsewhere too

The programmable company
-------------------------

We can (and should) see a company as a simplified robot - running an OODA loop.
Observe, Orient, Decide, Act
(Monitor, Model, Mentor,

But AI is able to *build* a model in a amazing new way,
but a model of the world is not useful without perception of
world and own ability.

So if AI can build some model of a "perfect manager brain",
it will still need to be fed perceptions of the operations daily

ANd are those already being fed upwards - honestly turns out mostly no.
THey are stuck in siloes, they are filter through powerpoint presrntations
and massaged by project managers,

AI can learn to balance a ball on a stick.  But if we have unclear
information about where the ball is, how the stick is moving etc,
it will perform at least as badly as an actual human manager.
The perception feedback must be good for any model to be transferrable.
If it is bad then how do we behave - the same way most managers behave -
find a few trusted lieutenants who can find *just enough* information
to be representative and keep churning and find that playing politics in
feudal environment is almost always more profitable than fixing the
perceptiin and action problems.

The better companies have a single powerful individual often founder,
who forces through feedback and action.  To me this indicates that
a successful company is built upwards from one winnign formulla, and then
left to drift aas it become unmanageable.

But if it were possible to make it programmable company, it may be possible to
rebuild it, guide it. Manage it. THorugh code.


AI is not magic. if you think some or all of job can be repaced by AI
then the question is, why cant it be repaced today woth traditional software

its not volume of data - no human can compete
its access to tools and ots decision politics



AI and the future of work
--------------------------

THis title is actually legally require din any Techn book published after 2020.
Sorry.


Will AI affect the future of work - yes, but not as much as "sky is falling",
and not as much as "robotic replacement" because companies nned to be software
literte - arranged so that one can iterate over them.  They need to programmable
- and if so then you dont need AI.  You just need to write code.  But then you
  get "free interns".

Role of training data, and journalism.  How dow e find ground truth in polarised
world.

Well world is always beenpolarised - see, Luther. We find it
in evidence, in OSINT, in reliable journlism - World Service being simplest
example of a massive foot gun for British people.

I would put Wikipedia as the
next battle ground - and how do we deal with that?
See also text books, and education syllabuses. 





HunterBrook - future if jountlaism
-----------------------------------

Journalism is not publishing. Publishing is now free
Jounralism is finding, investigating and surfacing the truth.
(that wont fix everything - see £ datys of condor and POst Office scandal)
BUt that act allows us to know what is "true".  It sets a *positive* standard
(as opposed to a negative standard such as Putin style make everything seem unbelievable)


yes I think, maybe
https://www.axios.com/2023/11/02/hunterbrook-hedge-fund-journalists
because we want to know what is "truth"
somehiw we need to pay for it - the paper based distribution mattered 
its disaggregated now but we still need scoiety level training day

some is science, NIH ans NICE
some is hunterbrook looking at OSINT
or just see hunterbrook as OsINT
aee the globe thing for snowcrash 
OSINT - the Ice Stati9on Zebra issue



Chapter: Journalism is labelling the training data for the world
=================================================================

Challenges of training data and bias
We started with 'easy' problems - facial recgnition and black african
descent. Oh look Stanford has white male phd students.

Bias in Generative AI: show me images of nazi stormtroopers.
Hang on. Why are there chinese or black african stomrtroopers?? Huh
Look at how skin cacner detection - is there a ruler in the image? Is the
image taken under flouresent light ?

- there is *almost certainly* child porn in training data. That bothers me
  enormously.

  - But what about  Fixing it. "publish your training data". Thats a *positive* move, but, "hey we
trained on these 5 billion images. What do you do with those? How do you even
classifiy them?

CV scanning. Anecodatally a large corporation decides to use AI to scan CVs,
and identify young people most likely to succeed in the corporation. It is given
the CVs of everyone in the company, and gets to work. It flatout rejects every
CV from a woman. They remove the gender from the CVs - it still does it.
They dig in - why is this going to be rejected. Basically, women reach a certain
point in the company, and rise no higher. Therefore women wont succeed at this
company.  Now what? It is correctly analysing the problem. Its not the answer
you want.

But it is a part of the democractic bet - AI is not fooled by the double-think
bias humans introduce to be able to survive.  Any totaltitarian regieme has that
in it.  But only an egalitarian democracy has the ability to change to make
itself truly equal.

Do we want to do that? THose who will obviously gain say yes. THose who will
lose, and what of those who will lose big? Shall we introduce a wealth tax?

World building matters (ability to plan is basically
ability to predict future. THis is a hall mark of intelligence - also why
people with bad internal models make poor decisions, and why its so hard to
get people with vastly differing models to understand each otehr - used to be
limited to crime. now... politics?  Its why its vital to edicate people to have
same model at first, its also why edication laevels make biggest
differentiator in politics, and also why choosing the first model makes your
'side' more relevant. See north korean education camps. But also see how
many people did nto believe societ model but kept stum'


Sympathy for the Facebook
--------------------------

timeline is the problem - sympathy for facebook because how dontounorgnaise
timeline ? cannot show eveything - cannot shownjust friends because broing so
whats the algorithm ? ask a go ernment they dont say just say "dont destroy
democeacy" but perhaps problem is "timeline" - dont do timeline do education or
agent with best interet s of the user


training data matters
---------------------

Google and pagerank soon became google and returned tonhomepage data as feedback
onnquality Tesla owns data on when the car braked or jerked or gas applied and
can record that and upload it nightly and use for modelling journlism is
societies way of marking training data textbooks are way of marking trianong
data science is way to doscover correct weights for feedback now why is it that
google keeps my clicks or my steering as ots own proprietary data health data -
it shoukd be public data by default licensing or otherwise but not unavailable

* Autism and rules software can represent, enfource, encourage, discover, speed
up rules. But rules that are written dwn threaten priviledge This antognism will
be paramount for future.  Piketty - can we beat him down? Can we over come
priviledge? SEC approach - legistlate priviledge into being good. But leave them
enough to make it worth thier while? THreat of digital currency. The example of
cryptocurrency as why we have regulatioon and crime (see Sherlock holmes stories
about bank failures)

autism and rukes - fristrationnof bureaucracy and lack of what rukes and where
to look it empowers thise innpower, but rikes take away power and priviledge
software is operationalmrules - the advantages outweigh cost of openness esp
when closed approach goves priviledge


sabine
------

. It is not an optimal device for intelligence because it's not what it evolved
to be. The human brain evolved to keep us alive. This means among other things
it needs to be energy efficient, which indeed it is quite good at.

But artificial brains have no such limits so of course they will eventually
outperform humans.

If you don't understand why superior intelligence is scary, you've read too many
touchy sci-fi stories in which human irrationality saves the day. But humans
didn't come to dominate this planet because they're somtimes irrational, they
came to dominate despite of it.

What use is intelligence (at this level). It’s making better decisions- more
informed (monitor) closer to reality (model) and more adhered to (mentor /
manage)

Companies are more intelligent at scale than the average and tail result for
each individual in the org would be if left alone (primarily thats adherence !)

So a more intelligent org has huge payoffs - that’s the software mind!!!

But also AI - even so at the level of species competition the quail is
technology is intelligence manifest in tools

Being more intelligent than any human is *fine* - but that’s only useful if up
against all other humans individually. If up against all humanity it’s a
different problem

No the real threat is use of AI by humans as a technology - surveillance,
democratic bet, weapon design etc

And yeah we can see a lot of that and the antidote is democracy and freedom 

The rest - what we cannot imagine - perhaps it will be like trying to explain
politics to a dog - at a certain point no question we ask or answer we get will
be meaningful

